epsilon_ber <- 2*rbinom(100,1,p=0.5)-1     # converting the values 0, 1 to -1, 1
x_ber <- cumsum(epsilon_ber)
plot(x_ber, type='l')
epsilon_norm <- rnorm(100, 0, sd=0.1)
x_norm <- cumsum(epsilon_norm)
plot(x_norm, type='l')
epsilon_norm <- rnorm(10000, 0, sd=0.1)
x_norm <- cumsum(epsilon_norm)
plot(x_norm, type='l')
epsilon_pois <- rpois(100, lambda=0.1)
x_pois <- cumsum(epsilon_pois)
plot(x_pois, type='l')
N<-10000
epsilon1_ber <-  2 * rbinom(N,1,p=0.5) - 1
epsilon2_ber <-  2 * rbinom(N,1,p=0.5) - 1
x_ber <-  cumsum(epsilon1_ber)
y_ber <-  cumsum(epsilon2_ber)
plot(x_ber, y_ber, type='l')
epsilon1_norm <-  rnorm(N,0,0.1)
epsilon2_norm <-  rnorm(N,0,0.1)
x_norm <-  cumsum(epsilon1_norm)
y_norm <-  cumsum(epsilon2_norm)
plot(x_norm, y_norm, type='l')
library(quantmod)
options("getSymbols.warning4.0"=FALSE)
sp500<-new.env()
startDate = as.Date('1960-01-04')
endDate   = as.Date('2018-11-24')
getSymbols('^GSPC',env=sp500, src='yahoo', from=startDate,
to=endDate, auto.assign=T)
install.packages("quantmod")
options("getSymbols.warning4.0"=FALSE)
sp500<-new.env()
startDate = as.Date('1960-01-04')
endDate   = as.Date('2018-11-24')
getSymbols('^GSPC',env=sp500, src='yahoo', from=startDate,
to=endDate, auto.assign=T)
library(quantmod)
options("getSymbols.warning4.0"=FALSE)
sp500<-new.env()
startDate = as.Date('1960-01-04')
endDate   = as.Date('2018-11-24')
getSymbols('^GSPC',env=sp500, src='yahoo', from=startDate,
to=endDate, auto.assign=T)
head(sp500$GSPC)
tail(sp500$GSPC)
class(sp500$GSPC)
sp_prc<-sp500$GSPC[,6]
class(sp_prc)    # xts, zoo      'zoo' stands for Z's ordered observations
#1960-01-04         59.91
#2018-11-23       2632.56
# total return    (2901.13-59.91)/59.91 ~ 47.4248
# daily return
plot(sp_prc, auto.grid=T)#, grid.ticks.on='M')
ret_sp500_daily<-(sp_prc-lag(sp_prc))/lag(sp_prc)
#or
ret_sp500_daily<-diff(sp_prc)/lag(sp_prc)
plot(ret_sp500_daily, auto.grid=T)#, grid.ticks.on='M')
mean(ret_sp500_daily,na.rm=T) * 252     # annualized return
# rollmean
sp_avg20 = rollmean(sp_prc, 20, align='center')
# rollmean
sp_avg20 = rollmean(sp_prc, 20, align='center')
plot(sp_avg20, auto.grid=T)#, grid.ticks.on='M')
# rollmean
sp_avg20 = rollmean(sp_prc, 20, align='center')
plot(sp_avg20, auto.grid=T)#, grid.ticks.on='M')
sp_avg100 = rollmean(sp_prc, 100, align='center')
plot(sp_avg100, auto.grid=T)#, grid.ticks.on='M')
ret_sp500_daily<-ret_sp500_daily[2:length(ret_sp500_daily)]  # removing the first nan element
ret_sp500_avg20 = rollmean(ret_sp500_daily, 20, align='center')
plot(ret_sp500_avg20, auto.grid=T)#, grid.ticks.on='M')
ret_sp500_avg100 = rollmean(ret_sp500_daily, 100, align='center')
plot(ret_sp500_avg100, auto.grid=T)#, grid.ticks.on='M') # The 100 trading days moving average is smoother than the 20 days version
# The daily return of SP500 has a big fat tail
qqnorm(ret_sp500_daily)
qqline(ret_sp500_daily)
N <- 100
sigma <- 0.1
epsilon = rnorm(N, 0, sigma)
X <- numeric(N)
theta <- 0.2
X[1]  <- 0.0
for (i in 2:N) {
X[i] <- theta * X[i-1] + epsilon[i]
}
plot(X,type='l')
N <- 100
sigma <- 0.1
epsilon = rnorm(N, 0, sigma)
Y <- numeric(N)
phi <- 0.6
Y[1]  <- 0.0
for (i in 2:N) {
Y[i] <- epsilon[i] + phi * epsilon[i-1]
}
plot(Y,type='l')
install.packages("tseries")
library(tseries)
# test for stationary
# adf.test
adf.test(sp_prc)
adf.test(ret_sp500_daily)
#
set.seed(0)
N <- 100
sigma <- 0.1
epsilon = rnorm(N, 0, sigma)
X <- numeric(N)
theta <- 0.2
X[1]  <- 2.0
for (i in 2:N) {
X[i] <- theta * X[i-1] + epsilon[i]
}
adf.test(X)
set.seed(0)
N <- 100
sigma <- 0.1
epsilon = rnorm(N, 0, sigma)
X <- numeric(N)
theta <- 0.5
X[1]  <- 2.0
for (i in 2:N) {
X[i] <- theta * X[i-1] + epsilon[i]
}
acf(X, lag.max=10)
# acf on the sp500 daily return
acf(ret_sp500_daily, lag.max=100)   # The stock index is generally efficient, there is no long term pattern
sp500_vola_daily <-rollapply(ret_sp500_daily, width=100, FUN=sd)
plot(sp500_vola_daily, auto.grid=T)#, grid.ticks.on='M')
# Some EDA on the relationship between volatility and thetas
return_vola_100 <- sp500_vola_daily[100:(M+99)]
return_vola     <- as.vector(return_vola_100)
return_autocorr <- as.vector(thetas)
return_daily_100    <- as.vector(rollmean(ret_sp500_daily, 100, align='right'))
plot(return_daily_100, return_vola, col=rgb(0,0,0, alpha=0.2))
# Some EDA on the relationship between volatility and thetas
return_vola_100 <- sp500_vola_daily[100:(M+99)]
return_vola     <- as.vector(return_vola_100)
result_theta<-rollapply(as.vector(ret_sp500_daily),width=100,FUN=acf,lag.max=1,
type='correlation',plot=FALSE, align='right')
# result_theta is a matrix
M<-length(result_theta[,1])
thetas <- numeric(M)
for (i in 1:M) {
thetas[i] <- result_theta[i,]$acf[2]
}
sp500_vola_daily <-rollapply(ret_sp500_daily, width=100, FUN=sd)
plot(sp500_vola_daily, auto.grid=T)#, grid.ticks.on='M')
thetas <- as.xts(thetas, order.by=index(sp500_vola_daily[100:(M+99)]))
plot(thetas, auto.grid=T)#, grid.ticks.on='M')
# Some EDA on the relationship between volatility and thetas
return_vola_100 <- sp500_vola_daily[100:(M+99)]
return_vola     <- as.vector(return_vola_100)
return_autocorr <- as.vector(thetas)
return_daily_100    <- as.vector(rollmean(ret_sp500_daily, 100, align='right'))
plot(return_daily_100, return_vola, col=rgb(0,0,0, alpha=0.2))
plot(return_vola, return_autocorr, col=rgb(0,0,0,alpha=0.2))
#install.packages('forecast')
library(forecast)
auto.arima(return_vola_100)  # default kpss test rejects the null hypothesis of stationary time series
install.packages("forecast")
#install.packages('forecast')
library(forecast)
auto.arima(return_vola_100)  # default kpss test rejects the null hypothesis of stationary time series
auto.arima(return_vola_100, test='adf')
sample <- seq(100, 14667, 100)
ret_vola_sampled <- return_vola_100[sample]
#Trace = T means Verbose = 1
auto.arima(ret_vola_sampled, test='adf', trace=T)  #ARIMA(1,0,0) with non-zero mean
# compared with
auto.arima(ret_vola_sampled, trace=T)  # ARIMA(1,1,1) by using the kpss test intead of adf test
auto.arima(return_vola_100, test='adf')
sample <- seq(100, 14667, 100)
ret_vola_sampled <- return_vola_100[sample]
#Trace = T means Verbose = 1
auto.arima(ret_vola_sampled, test='adf', trace=T)  #ARIMA(1,0,0) with non-zero mean
# compared with
auto.arima(ret_vola_sampled, trace=T)  # ARIMA(1,1,1) by using the kpss test intead of adf test
#
auto.arima(ret_vola_sampled^2, test='adf', trace=T)   # Don't forget Box-Cox!
# The AR coefficient drops in the new model
acf(ret_vola_sampled^2)
#AR(2) model
N <- 1000
epsilon <- rnorm(N, 0, 0.1)
X <- numeric(N)
for (i in 3:N) {
X[i] <- 0.5 * X[i-2] + epsilon[i]
}
# compare acf vs pacf
acf(X, lag.max=20)
pacf(X, lag.max=20)
arima(sp_prc, order=c(1,1,0))
# compute the theoretical ACF from an ARMA TS
plot(ARMAacf(ar = 0, ma=c(1.0, -0.5), lag.max=20, pacf=FALSE))
acf(X, lag.max=20)  # Notice that acf drops to nearly zero after (including) k=3
# compute the theoretical ACF from an ARMA TS
plot(ARMAacf(ar = 0, ma=c(1.0, -0.5), lag.max=20, pacf=FALSE))
arima(sp_prc, order=c(1,1,1))
fit<-arima(sp_prc, order=c(2,1,0))
summary(fit)
output<-forecast(fit,h=252)
plot(output)
names(output)
head(output$lower)
head(output$upper)
#AR(2) model
N <- 1000
epsilon <- rnorm(N, 0, 0.1)
X <- numeric(N)
for (i in 3:N) {
X[i] <- 0.5 * X[i-2] + epsilon[i]
}
# compare acf vs pacf
acf(X, lag.max=20)
pacf(X, lag.max=20)
pacf(X, lag.max=20)
# compare acf vs pacf
acf(X, lag.max=20)
fit <- auto.arima(sp_prc, test='adf')
summary(fit)
fitted.values(fit)
residuals(fit)
output<-forecast(fit,h=252)
plot(output)
# On the ACF of an MA(2) series
N <- 1000
epsilon <- rnorm(N, 0, 0.1)
X <- numeric(N)
for (i in 3:N) {
X[i] <- epsilon[i] + 1.0 * epsilon[i-1] - 0.5 * epsilon[i-2]
}
acf(X, lag.max=20)  # Notice that acf drops to nearly zero after (including) k=3
# compute the theoretical ACF from an ARMA TS
plot(ARMAacf(ar = 0, ma=c(1.0, -0.5), lag.max=20, pacf=FALSE))
setwd("C:/Users/rayl1/Desktop/NYC Data Science Academy/Bootcamp/Week_10/ab_test/data")
library(dplyr)
library(tidyr)
test_CO <- read.csv("test_co.csv")
test_wl <- read.csv("test_wl.csv")
test_CO <- read.csv("test_co.csv")
before_wl <- read.csv("before_wl.csv")
before_CO <- read.csv("before_co.csv")
after_wl <- read.csv("after_wl.csv")
after_CO <- read.csv("after_co.csv")
View(after_CO)
View(after_wl)
merge(after_CO, after_wl, by = "theugid")
View(after_wl)
View(test_wl)
View(before_wl)
library(dplyr)
library(stringr)# to count N, L and C
before_wl <- read.csv("./data/before_wl.csv")
before_co <- read.csv("./data/before_co.csv")
test_co <- read.csv("./data/test_co.csv")
test_wl <- read.csv("./data/test_wl.csv")
setwd("C:/Users/rayl1/Desktop/NYC Data Science Academy/Bootcamp/Week_10/ab_test")
before_wl <- read.csv("./data/before_wl.csv")
before_co <- read.csv("./data/before_co.csv")
test_co <- read.csv("./data/test_co.csv")
test_wl <- read.csv("./data/test_wl.csv")
after_co <- read.csv("./data/after_co.csv")
after_wl <- read.csv("./data/after_wl.csv")
library(dplyr)
library(stringr)# to count N, L and C
before_wl <- read.csv("./data/before_wl.csv")
before_co <- read.csv("./data/before_co.csv")
test_co <- read.csv("./data/test_co.csv")
test_wl <- read.csv("./data/test_wl.csv")
after_co <- read.csv("./data/after_co.csv")
after_wl <- read.csv("./data/after_wl.csv")
View(test_co)
clean.co<-function(x,time){
tab<-x%>%
group_by(theugid)%>%
mutate(reg_times = sum(reg_success))%>%
mutate(Status=ifelse(reg_times>0,"Create","Lost"))
tab$Phase=time
col=if(time!="Test"){
c("thedevice","firstpage","Status" ,"Phase")
}else{
c("thedevice","Status","firstpage","Phase","themodule")
}
tab=tab[,col]
return(tab)
}
clean.co<-function(x,time){
tab<-x%>%
group_by(theugid)%>%
mutate(reg_times = sum(reg_success))%>%
mutate(Status=ifelse(reg_times>0,"Create","Lost"))
tab$Phase=time
col=if(time!="Test"){
c("thedevice","firstpage","Status" ,"Phase")
}else{
c("thedevice","Status","firstpage","Phase","themodule")
}
tab=tab[,col]
return(tab)
}
clean.wl<-function(x,time){
# check the status
x$number.of.l <- str_count(x$login_or_create, "L")
x$number.of.c <- str_count(x$login_or_create, "C")
x$number.of.user <-x$eventcount-str_count(x$isloggedin_r, "by-session")
x$creat_s<-ifelse(x$number.of.c>0 & x$number.of.user>0,1,0)
x$creat_f<-ifelse(x$number.of.c>0 & x$number.of.user==0,1,0)
x$return_s <- ifelse(x$number.of.c==0 & x$number.of.l>0 & x$number.of.user>0,1,0)
x$return_f <- ifelse(x$number.of.c==0 & x$number.of.l>0 & x$number.of.user==0,1,0)
# add one column to indicate the status
tbl<-x%>%
group_by(theugid)%>%
mutate(cs_times = sum(creat_s),
cf_times = sum(creat_f),
rs_times = sum(return_s),
rf_times = sum(return_f))%>%
mutate(Status=ifelse(cs_times>0,
"Create_S",
ifelse(cf_times>0,
"Create_F",
ifelse(rs_times>0,
"Return_S",
ifelse(rf_times>0,
"Return_F",
"Lost"
)))))
tbl$Phase=time
col=if(time!="Test"){
c("thedevice","firstpage","Status" ,"Phase")
}else{
c("thedevice","Status","firstpage","Phase","themodule")
}
tbl=tbl[,col]
return(tbl)
}
clean.wl.bi<-function(x,time){
tbl=clean.wl(x,time)
tbl=tbl%>%
filter(Status %in% c("Create_S","Create_F"))
return(tbl)
}
clean.combine<-function(x,time1,y,time2,z,time3,function1){
DT1=function1(x,time1)
DT2=function1(y,time2)
DT3=function1(z,time3)
Final=as.data.frame(bind_rows(DT1,DT2,DT3))
Final=as.data.frame(sapply(Final,as.factor)) # sapply won't work?????
Final$Status<- relevel(Final$Status,
ref = ifelse(as.character(substitute(function1))=="clean.co",
"Create",
"Create_S"))
Final$Phase <- relevel(Final$Phase, ref = "Test")
Final$themodule <- relevel(Final$themodule, ref = "F")
return(Final)
}
co_final=clean.combine(before_co,"Before",test_co,"Test",after_co,"After",clean.co)
wl_final=clean.combine(before_wl,"Before",test_wl,"Test",after_wl,"After",clean.wl)
wl_final_bi=clean.combine(before_wl,"Before",test_wl,"Test",after_wl,"After",clean.wl.bi)
View(clean.co)
View(wl_final)
View(wl_final)
View(wl_final_bi)
View(co_final)
wl_final_bi %>%
select(Status) %>%
distinct()
library(ggplot2)
wl_final_bi %>%
groupby(module) %>%
summarise(count(Status))
wl_final_bi %>%
group_by(module) %>%
summarise(count(Status))
wl_final_bi %>%
group_by(themodule) %>%
summarise(count(Status))
wl_final_bi %>%
group_by(themodule) %>%
summarise(n(status))
wl_final_bi %>%
group_by(themodule, status) %>%
summarise(n())
wl_final_bi %>%
group_by(themodule, Status) %>%
summarise(n())
wl_final_bi %>%
group_by(themodule, Status) %>%
summarise(n()) %>%
ggplot(aes(themodule,"n()")) +
geom(bar)
wl_final_bi %>%
group_by(themodule, Status) %>%
summarise(n()) %>%
ggplot(aes(themodule,"n()")) +
geom_bar()
wl_final_bi %>%
group_by(themodule, Status) %>%
summarise(n()) %>%
ggplot(aes("n()")) +
geom_bar(fill="themodule")
wl_final_bi %>%
group_by(themodule, Status) %>%
summarise(n()) %>%
ggplot(aes("n()", fill="themodule")) +
geom_bar()
wl_final_bi %>%
group_by(themodule, Status) %>%
summarise(n()) %>%
ggplot(aes(y="n()", x="themodule", stat="identity")) +
geom_bar()
wl_final_bi %>%
group_by(themodule, Status) %>%
summarise(n()) %>%
ggplot(aes(y="n()", x="themodule", stat="identity")) +
geom_bar()
wl_final_bi %>%
group_by(themodule, Status) %>%
summarise(n()) %>%
ggplot(aes(y="n()", x="themodule")) +
geom_bar(stat="identity")
wl_final_bi %>%
group_by(themodule, Status) %>%
summarise(n()) %>%
ggplot(aes(x="themodule",y="n()")) +
geom_col()
wl_final_bi %>%
group_by(themodule, Status) %>%
summarise(n()) %>%
ggplot(aes(x="themodule",y="n()")) +
geom_col()
wl_final_bi %>%
group_by(themodule, Status) %>%
summarise(n()) %>%
ggplot(aes(x=themodule,y=n())) +
geom_col()
wl_final_bi %>%
group_by(themodule, Status) %>%
summarise(n()) %>%
ggplot(aes(x=themodule,y="n()")) +
geom_col()
wl_final_bi %>%
group_by(themodule, Status) %>%
summarise(n())-> test
ggplot(test,aes(x=themodule,y="n()")) +
geom_col()
ggplot(test) +
geom_col(aes(x=themodule,y="n()"))
library(dplyr)
library(ggplot2)
### Comparing creation probability after checking out
co_final %>% head()
co_final %>%
mutate(Create=(Status=='Create'), social=(Phase=='Test'))%>%
group_by(social) %>%
summarise(Ratio = sum(Create/n()))
wl_final_bi %>%
group_by(themodule, Status) %>%
summarise(n()) %>%
ggplot(aes(x=themodule,y=n())) +
geom_bar()
wl_final_bi %>%
group_by(themodule, Status) %>%
summarise(n()) %>%
ggplot(aes(x=themodule,y=n())) +
geom_bar()
ggplot(aes(x=themodule,y=count) +
geom_bar()
wl_final_bi %>%
group_by(themodule, Status) %>%
summarise(count = n()) %>%
ggplot(aes(x=themodule,y=count)) +
geom_bar()
wl_final_bi %>%
group_by(themodule, Status) %>%
summarise(count = n())
wl_final_bi %>%
group_by(themodule, Status) %>%
summarise(count = n()) %>%
ggplot() +
geom_bar(aes(x=themodule,y=count))
wl_final_bi %>%
group_by(themodule, Status) %>%
summarise(count = n()) %>%
ggplot() +
geom_bar(aes(x=themodule,y=count), stat="identity")
setwd("C:/Users/rayl1/Desktop/NYC Data Science Academy/Bootcamp/Project/Capstone_NOSO/NOSO/Ray")
